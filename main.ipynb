{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "007cc86f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "479b4223",
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(logits):\n",
    "    stabalized = logits - np.max(logits, axis = 1, keepdims = True)\n",
    "    expo_values = np.exp(stabalized)\n",
    "    sft_values = expo_values/ np.sum(expo_values, axis = 1, keepdims = True)\n",
    "    return sft_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5e288297",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prob(X, W, b):\n",
    "    logits = np.matmul(X,W) + b \n",
    "    prob = softmax(logits)\n",
    "    return prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ed0b3592",
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot(y, num_classes):\n",
    "    N = y.shape[0]\n",
    "    oh = np.zeros((N, num_classes))\n",
    "    oh[np.arange(N), y ] = 1.0\n",
    "    return oh\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "fad6889f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def categorical_cross_entropy(y_hat, y):\n",
    "    N = len(y)\n",
    "    column_indices = y\n",
    "    values = y_hat[np.arange(N), column_indices]\n",
    "    stabalized_values = values + 1e-8\n",
    "    cce = -(np.log(stabalized_values))\n",
    "    return  np.mean(cce)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "0e135132",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_comp(x,y,w,b, num_classes):\n",
    "    N = len(y)\n",
    "    probs = prob(x,w,b)\n",
    "    dlogits = (probs - one_hot(y, num_classes))/ N\n",
    "    dw = np.matmul(x.T, dlogits)\n",
    "    db = np.sum(dlogits, axis = 0, keepdims = True)\n",
    "    return dw, db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d002c68c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descent(x, y, num_classes, epochs = 10000, lr = 0.01):\n",
    "    N,D = x.shape\n",
    "    w = np.random.normal(scale= 0.01, size = (D,num_classes))\n",
    "    b = np.random.normal(scale = 0.01, size = (1, num_classes))\n",
    "    for i in range (epochs+1):\n",
    "        dw, db = gradient_comp(x,y,w,b, num_classes)\n",
    "        w -= lr*dw\n",
    "        b-= lr*db\n",
    "        if i % 100 ==0:\n",
    "            print(f\"the loss after {i}th iteration is {categorical_cross_entropy(prob(x, w, b), y)} \")\n",
    "    return w,b\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "56dc141e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>petal_length</th>\n",
       "      <th>petal_width</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2.037719</td>\n",
       "      <td>0.486790</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2.192127</td>\n",
       "      <td>0.510490</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.839299</td>\n",
       "      <td>0.536160</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2.391200</td>\n",
       "      <td>0.594708</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.788879</td>\n",
       "      <td>0.373458</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   petal_length  petal_width  class\n",
       "0      2.037719     0.486790      0\n",
       "1      2.192127     0.510490      0\n",
       "2      1.839299     0.536160      0\n",
       "3      2.391200     0.594708      0\n",
       "4      1.788879     0.373458      0"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv(\"flowers.csv\")\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "560e11b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.array(data.iloc[:, :-1])\n",
    "y = np.array(data.iloc[:, -1])\n",
    "num_classes = 3\n",
    "scaler = StandardScaler()\n",
    "x_processed = scaler.fit_transform(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "332d1a34",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the loss after 0th iteration is 1.1131451178398704 \n",
      "the loss after 100th iteration is 0.7336926894047974 \n",
      "the loss after 200th iteration is 0.5966240323484624 \n",
      "the loss after 300th iteration is 0.5228842970014502 \n",
      "the loss after 400th iteration is 0.4723999332043573 \n",
      "the loss after 500th iteration is 0.43334807032742256 \n",
      "the loss after 600th iteration is 0.4012044367505507 \n",
      "the loss after 700th iteration is 0.3738333842094445 \n",
      "the loss after 800th iteration is 0.3500440601606884 \n",
      "the loss after 900th iteration is 0.3290837337905952 \n",
      "the loss after 1000th iteration is 0.3104323028362574 \n",
      "the loss after 1100th iteration is 0.29370768555286086 \n",
      "the loss after 1200th iteration is 0.27861705504825246 \n",
      "the loss after 1300th iteration is 0.264929131524476 \n",
      "the loss after 1400th iteration is 0.25245713161560523 \n",
      "the loss after 1500th iteration is 0.24104759694433975 \n",
      "the loss after 1600th iteration is 0.23057271195029908 \n",
      "the loss after 1700th iteration is 0.2209248192238337 \n",
      "the loss after 1800th iteration is 0.21201238570434702 \n",
      "the loss after 1900th iteration is 0.20375696347679328 \n",
      "the loss after 2000th iteration is 0.19609085347269725 \n",
      "the loss after 2100th iteration is 0.1889552786565243 \n",
      "the loss after 2200th iteration is 0.18229893453404056 \n",
      "the loss after 2300th iteration is 0.17607682436271105 \n",
      "the loss after 2400th iteration is 0.17024931272294794 \n",
      "the loss after 2500th iteration is 0.16478134900788466 \n",
      "the loss after 2600th iteration is 0.15964182484471665 \n",
      "the loss after 2700th iteration is 0.15480303829526476 \n",
      "the loss after 2800th iteration is 0.1502402440593709 \n",
      "the loss after 2900th iteration is 0.14593127357992733 \n",
      "the loss after 3000th iteration is 0.14185621242694463 \n",
      "the loss after 3100th iteration is 0.13799712496146707 \n",
      "the loss after 3200th iteration is 0.13433781828333313 \n",
      "the loss after 3300th iteration is 0.1308636390139272 \n",
      "the loss after 3400th iteration is 0.12756129767258928 \n",
      "the loss after 3500th iteration is 0.12441871635685643 \n",
      "the loss after 3600th iteration is 0.1214248961930845 \n",
      "the loss after 3700th iteration is 0.11856980163008066 \n",
      "the loss after 3800th iteration is 0.11584425913759527 \n",
      "the loss after 3900th iteration is 0.11323986826905748 \n",
      "the loss after 4000th iteration is 0.11074892337295253 \n",
      "the loss after 4100th iteration is 0.10836434450446404 \n",
      "the loss after 4200th iteration is 0.10607961630985983 \n",
      "the loss after 4300th iteration is 0.1038887338394921 \n",
      "the loss after 4400th iteration is 0.10178615439826312 \n",
      "the loss after 4500th iteration is 0.09976675467053334 \n",
      "the loss after 4600th iteration is 0.09782579246417261 \n",
      "the loss after 4700th iteration is 0.09595887250936427 \n",
      "the loss after 4800th iteration is 0.09416191582473468 \n",
      "the loss after 4900th iteration is 0.09243113222876273 \n",
      "the loss after 5000th iteration is 0.09076299563012233 \n",
      "the loss after 5100th iteration is 0.08915422177820588 \n",
      "the loss after 5200th iteration is 0.0876017481958545 \n",
      "the loss after 5300th iteration is 0.08610271605134762 \n",
      "the loss after 5400th iteration is 0.08465445375687294 \n",
      "the loss after 5500th iteration is 0.08325446210673619 \n",
      "the loss after 5600th iteration is 0.08190040079110221 \n",
      "the loss after 5700th iteration is 0.08059007614059568 \n",
      "the loss after 5800th iteration is 0.07932142997406702 \n",
      "the loss after 5900th iteration is 0.07809252943661386 \n",
      "the loss after 6000th iteration is 0.07690155772784754 \n",
      "the loss after 6100th iteration is 0.07574680563167016 \n",
      "the loss after 6200th iteration is 0.07462666376870443 \n",
      "the loss after 6300th iteration is 0.0735396155011838 \n",
      "the loss after 6400th iteration is 0.07248423042772772 \n",
      "the loss after 6500th iteration is 0.07145915841213228 \n",
      "the loss after 6600th iteration is 0.07046312409622116 \n",
      "the loss after 6700th iteration is 0.06949492185202659 \n",
      "the loss after 6800th iteration is 0.06855341113319156 \n",
      "the loss after 6900th iteration is 0.06763751218958065 \n",
      "the loss after 7000th iteration is 0.06674620211272075 \n",
      "the loss after 7100th iteration is 0.06587851118292462 \n",
      "the loss after 7200th iteration is 0.06503351949182253 \n",
      "the loss after 7300th iteration is 0.06421035381659101 \n",
      "the loss after 7400th iteration is 0.06340818472445335 \n",
      "the loss after 7500th iteration is 0.06262622388806856 \n",
      "the loss after 7600th iteration is 0.06186372159425569 \n",
      "the loss after 7700th iteration is 0.06111996443013536 \n",
      "the loss after 7800th iteration is 0.060394273132241355 \n",
      "the loss after 7900th iteration is 0.05968600058547285 \n",
      "the loss after 8000th iteration is 0.05899452995994552 \n",
      "the loss after 8100th iteration is 0.05831927297486525 \n",
      "the loss after 8200th iteration is 0.05765966827951293 \n",
      "the loss after 8300th iteration is 0.05701517994229546 \n",
      "the loss after 8400th iteration is 0.05638529603960191 \n",
      "the loss after 8500th iteration is 0.05576952733691378 \n",
      "the loss after 8600th iteration is 0.055167406055259334 \n",
      "the loss after 8700th iteration is 0.05457848471668248 \n",
      "the loss after 8800th iteration is 0.05400233506292638 \n",
      "the loss after 8900th iteration is 0.053438547042006686 \n",
      "the loss after 9000th iteration is 0.052886727857789084 \n",
      "the loss after 9100th iteration is 0.05234650107807579 \n",
      "the loss after 9200th iteration is 0.05181750579707181 \n",
      "the loss after 9300th iteration is 0.051299395848424315 \n",
      "the loss after 9400th iteration is 0.050791839065331544 \n",
      "the loss after 9500th iteration is 0.05029451658448996 \n",
      "the loss after 9600th iteration is 0.049807122190896495 \n",
      "the loss after 9700th iteration is 0.04932936170075358 \n",
      "the loss after 9800th iteration is 0.04886095237993059 \n",
      "the loss after 9900th iteration is 0.048401622395629694 \n",
      "the loss after 10000th iteration is 0.04795111029907718 \n"
     ]
    }
   ],
   "source": [
    "W,b = gradient_descent(x_processed,y,num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51d6d074",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted classes: [[9.76434587e-01 2.35648862e-02 5.27003622e-07]\n",
      " [4.66251508e-02 9.32135164e-01 2.12396852e-02]\n",
      " [3.73421178e-07 1.58361075e-02 9.84163519e-01]]\n"
     ]
    }
   ],
   "source": [
    "test_flowers = np.array([\n",
    "        [2.1, 0.4],\n",
    "        [4.0, 1.2],\n",
    "        [6.2, 2.1],\n",
    "    ])\n",
    "   \n",
    "preds = prob(scaler.transform(test_flowers), W, b)\n",
    "print(\"Predicted classes:\", preds)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
